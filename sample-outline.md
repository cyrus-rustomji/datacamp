# Sample Course Outline


## Chapter 1 - Most commonly used Regressions
* Lesson 1 - Breaking down Linear Regression
	* Learning objective: Understanding things to watch out for, memory, hyperparameters and more for Linear Regressions
* Lesson 2 - Breaking down Logistic Regression
	* Learning objective: Understanding things to watch out for, memory, hyperparameters and more for Logistic Regressions
* Lesson 3 - When to use these?
	* Learning objective: Real world examples of when to use each regression

	
## Chapter 2 - Deep dive into Classification Models
* Lesson 1 - Breaking down SVMs
	* Learning objective: Understanding things to watch out for, memory, hyperparameters and more for SVMs
* Lesson 2 - Breaking down KNN
	* Learning objective: Understanding things to watch out for, memory, hyperparameters and more for KNN
* Lesson 3 - 	Breaking down Naive Bayes
	* Learning objective: Understanding things to watch out for, memory, hyperparameters and more for Naive Bayes
* Lesson 4 - 	Breaking down Decision Trees and Random Forests
	* Learning objective: Understanding things to watch out for, memory, hyperparameters and more for Decision Trees and Random Forests

	
## Chapter 3 - Model Evalutation
* Lesson 1 - R^2 and Adjusted R^2
	* Learning objective: Understanding the difference in linear regression metrics
* Lesson 2 - Understanding a confusion matrix
	* Learning objective: Learning when to show recall, precision, sensitivity, and specificity when working with classification models
* Lesson 3 - Understanding AUC and ROC Curve
	* Learning objective: Understand how AUC and ROC curves help give your data a better story
* Lesson 4 - Why RMSE?
	* Learning objective: Understand why RMSE is important when working with neural networks

	
## Chapter 4 - Natural Language Processing Overview
* Lesson 1 - Why pre-process data?
	* Learning objective: Given documents, how can you represent the words in a more condensed form?
* Lesson 2 - Targeting dimension reduction
	* Learning objective: Given the vectorized and condensed words, how can you quantify patterns and similarities in the documents?
* Lesson 3 - How to integrate a model with your cleaned data?
	* Learning objective: Given those patterns and similarities, how can you apply that to a function?