{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T08:48:22.449283Z",
     "start_time": "2019-01-31T08:48:22.446115Z"
    }
   },
   "source": [
    "# Final Lesson of the Final Chapter's Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T09:01:10.521670Z",
     "start_time": "2019-01-31T09:01:10.511172Z"
    }
   },
   "source": [
    "## Chapter 4 - Natural Language Processing Overview\n",
    "> Lesson 3 - \n",
    "    > \n",
    "    > Learning objective: Given those patterns and similarities, how can you apply that to a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Congrats! You've made it this far, one more exercise left! In this final exercise, we will pick which model fits our dataset best and why.\n",
    "\n",
    "> We will pick which model will work best for our data, then build a model, and then evaluate the results!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Which model would be best to use if your dataset is whether a tweet is referring to a \n",
    "disastrous or an irrelevant event? \n",
    "\n",
    "a. SVM\n",
    "b. Logistic Regression\n",
    "c. Naive Bayes\n",
    "d. KNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hint 1: The best model to use would be one that outputs probabilities.\n",
    "Hint 2: The best model to use would be a regression."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "answer: b. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, build a confusion matrix for this dataset (this will have prepopulated data for the student to understand the problem)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def plot_confusion_matrix(cm,title='Confusion matrix', cmap=plt.cm.Reds):\n",
    "    plt.imshow(cm, interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Second, build your model. Fill in the blanks."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_score(classifier,x,y):\n",
    "    xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x, y, test_size=0.2, random_state=1234)\n",
    "    ytrain=np.ravel(answer_3)\n",
    "    clf = classifier.fit(xtrain, ytrain)\n",
    "    # accuracy for test & train:\n",
    "    train_acc=clf.score(xtrain, ytrain)\n",
    "    test_acc=clf.score(xtest,ytest)\n",
    "    print(\"Training Data Accuracy: %0.2f\" %(train_acc))\n",
    "    print(\"Test Data Accuracy:     %0.2f\" %(test_acc))\n",
    "    \n",
    "    y_true = ytest\n",
    "    y_pred = clf.predict(xtest)\n",
    "\n",
    "\n",
    "    conf = confusion_matrix(answer_1, y_pred)\n",
    "    print(conf)\n",
    "\n",
    "    print ('\\n')\n",
    "    print (\"Precision:              %0.2f\" %(conf[0, 0] / (conf[0, 0] + conf[1, 0])))\n",
    "    print (\"Recall:                 %0.2f\"% (conf[0, 0] / (conf[0, 0] + conf[0, 1])))\n",
    "    \n",
    "    cm=confusion_matrix(y_true, y_pred, labels=None)\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    " \n",
    "log_clf=LogisticRegression()\n",
    "train_score(log_clf,X,y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "question 1 solutions:\n",
    "    answer_1 = xtest\n",
    "    answer_2 = xtrain\n",
    "    answer_3 = ytrain\n",
    "    answer_4 = xtrain\n",
    "    \n",
    "question 2 solutions:\n",
    "    answer_1 = y_true\n",
    "    answer_2 = y_pred\n",
    "    answer_3 = x_true\n",
    "    answer_4 = xtest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The accuracy score of this model is 85%, precision score is 90%, sensitivity is 84%, and \n",
    "specificity is 87%. Which score would you pick if you want to only show the true posivites \n",
    "over the true positives and false positives?\n",
    "\n",
    "a. accuracy score\n",
    "b. precision\n",
    "c. sensitivity\n",
    "d. specificity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hint 1: Tp / Tp + Fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer: b. precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
